# -*- coding: utf-8 -*-
"""refined-historical-events-api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14kU4LuB-tEAko6T485H7_WFmGDdSEU3X
"""

!pip install requests
!pip install nltk
!pip install skyfield

!pip install requests skyfield nltk spacy
!python -m spacy download en_core_web_sm

import json
import requests
from datetime import datetime
import spacy
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

# Load SpaCy model
!python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm")

# Define planetary symbolism categories
symbolic_categories = {
    'commerce': ['commerce', 'trade', 'business', 'economy'],
    'communication': ['communication', 'media', 'news', 'speech'],
    'war': ['war', 'conflict', 'battle', 'military'],
    'travel': ['travel', 'exploration', 'migration', 'transportation'],
    'conflict': ['conflict', 'dispute', 'tension', 'hostility'],
    'action': ['action', 'movement', 'activity', 'initiative'],
    'law': ['law', 'legal', 'court', 'justice'],
    'philosophy': ['philosophy', 'thought', 'belief', 'ideology'],
    'expansion': ['expansion', 'growth', 'development', 'increase'],
    'structure': ['structure', 'organization', 'institution', 'framework'],
    'discipline': ['discipline', 'control', 'regulation', 'order'],
    'government': ['government', 'state', 'policy', 'administration'],
    'innovation': ['innovation', 'invention', 'technology', 'discovery'],
    'revolution': ['revolution', 'uprising', 'rebellion', 'change'],
    'spirituality': ['spirituality', 'religion', 'faith', 'belief'],
    'dreams': ['dreams', 'visions', 'aspirations', 'imagination'],
    'illusion': ['illusion', 'deception', 'delusion', 'mirage'],
    'transformation': ['transformation', 'change', 'shift', 'metamorphosis'],
    'power': ['power', 'authority', 'control', 'dominance'],
    'rebirth': ['rebirth', 'renewal', 'revival', 'resurrection']
}

# Function to fetch Wikipedia events for a given year
def fetch_wikipedia_events(year):
    url = f'https://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&explaintext=true&titles={year}'
    response = requests.get(url)
    json_data = response.json()

    pages = json_data.get('query', {}).get('pages', {})
    extract = ""
    for page_id in pages:
        if 'extract' in pages[page_id]:
            extract = pages[page_id]['extract']

    return extract.strip() if extract else "No relevant information found."

# Function to categorize events based on symbolic categories
def categorize_events(text):
    doc = nlp(text.lower())
    categorized_events = {category: [] for category in symbolic_categories}

    for sentence in doc.sents:
        for category, keywords in symbolic_categories.items():
            if any(keyword in sentence.text for keyword in keywords):
                categorized_events[category].append(sentence.text.capitalize())

    # Return only categories with at least one event
    return {k: v for k, v in categorized_events.items() if v}

# Main function to fetch and categorize events for the past 100 years
def main():
    current_year = datetime.now().year
    historical_events = {}

    for year in range(current_year - 100, current_year):
        logging.info(f"Fetching events for the year {year}")
        wikipedia_text = fetch_wikipedia_events(year)
        categorized_events = categorize_events(wikipedia_text)
        historical_events[year] = categorized_events

        # Save each year's events to a file (this part should be inside the loop)
        with open(f'{year}_events.json', 'w') as f:
            json.dump(categorized_events, f, indent=4)

    # Save all events to a single file (optional)
    with open('historical_events_100_years.json', 'w') as f:
        json.dump(historical_events, f, indent=4)

if __name__ == "__main__":
    main()

def clean_text(text):
    # Replace unwanted characters
    clean_text = text.replace("\\u2013", "â€“")  # Replace unicode dash with actual dash
    clean_text = clean_text.replace("\\n", " ")  # Replace newlines with a space
    clean_text = clean_text.replace("==", "")  # Remove "==" sections
    clean_text = clean_text.replace("\\u00", "")  # Remove unnecessary unicode sequences
    clean_text = ' '.join(clean_text.split())  # Remove extra spaces

    # Optional: You can add more replacements or removals if needed

    return clean_text

def categorize_events(text):
    doc = nlp(text.lower())
    categorized_events = {category: [] for category in symbolic_categories}

    for sentence in doc.sents:
        clean_sentence = clean_text(sentence.text.strip())

        for category, keywords in symbolic_categories.items():
            if any(keyword in clean_sentence for keyword in keywords):
                categorized_events[category].append(clean_sentence.capitalize())

    # Return only categories with at least one event
    return {k: v for k, v in categorized_events.items() if v}

def main():
    current_year = datetime.now().year
    historical_events = {}

    for year in range(current_year - 100, current_year):
        logging.info(f"Fetching events for the year {year}")
        wikipedia_text = fetch_wikipedia_events(year)
        categorized_events = categorize_events(wikipedia_text)
        historical_events[year] = categorized_events

    # Save cleaned and formatted data to a new JSON file
    with open('cleaned_historical_events_100_years.json', 'w') as f:
        json.dump(historical_events, f, indent=4)

    logging.info("Cleaned data saved to 'cleaned_historical_events_100_years.json'")

if __name__ == "__main__":
    main()